#!/usr/bin/env python3
"""
Download liked tweets from X to Markdown and HTML.

This script uses Playwright to open your likes page and each tweet.
It produces one .md and one .html file per liked tweet.
"""
from __future__ import annotations

import argparse
import html
import os
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional, Sequence, Set, Tuple
from urllib.parse import parse_qsl, urlencode, urljoin, urlparse, urlunparse

try:  # pragma: no cover - optional import
    from playwright.sync_api import TimeoutError as PlaywrightTimeoutError, sync_playwright
except ImportError:  # pragma: no cover - environment without Playwright
    PlaywrightTimeoutError = RuntimeError  # type: ignore[misc,assignment]
    sync_playwright = None  # type: ignore[assignment]

try:  # pragma: no cover - optional import
    import markdown as md_lib
except ImportError:  # pragma: no cover - environment without markdown
    md_lib = None

DEFAULT_LIKES_URL = os.environ.get("TWEET_LIKES_URL", "")
DEFAULT_MAX_TWEETS = int(os.environ.get("TWEET_LIKES_MAX", "50"))
DEFAULT_STATE_PATH = os.environ.get("TWEET_LIKES_STATE", "x_state.json")
DEFAULT_DEST_DIR = os.environ.get("TWEET_LIKES_DEST", "liked_tweets")
WAIT_MS = 1000
TWEET_DETAIL_WAIT_MS = 5000
SHOW_MORE_WAIT_MS = 600

USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15"
)
STEALTH_SNIPPET = """
Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
window.chrome = window.chrome || { runtime: {} };
const originalQuery = window.navigator.permissions.query;
window.navigator.permissions.query = (parameters) => (
  parameters.name === 'notifications'
    ? Promise.resolve({ state: Notification.permission })
    : originalQuery(parameters)
);
"""


@dataclass(frozen=True)
class LikeTweet:
    url: str
    author_handle: str | None = None
    author_name: str | None = None
    time_text: str | None = None
    time_datetime: str | None = None


@dataclass(frozen=True)
class TweetParts:
    author_name: str | None
    author_handle: str | None
    body_text: str
    avatar_url: str | None
    trailing_media_lines: List[str]
    media_present: bool
    external_link: str | None


ELLIPSIS = "\u2026"
MIDDLE_DOT = "\u00b7"

FRONT_MATTER_KEYS = {
    "source": "tweet-source",
    "tweet_url": "tweet-url",
    "tweet_author": "tweet-author",
    "tweet_author_name": "tweet-author-name",
}

BASE_CSS = (
    "body { margin: 6%; font-family: -apple-system, BlinkMacSystemFont, "
    "'Segoe UI', Roboto, sans-serif; line-height: 1.6; }\n"
    "h1, h2, h3 { font-weight: bold; border-bottom: 1px solid #eee; "
    "padding-bottom: 8px; }\n"
    "blockquote { margin-left: 0; padding-left: 16px; border-left: 3px solid #ddd; "
    "color: #555; }\n"
    "a { color: #0a58ca; text-decoration: none; }\n"
    "a:hover { text-decoration: underline; }\n"
    "hr { border: none; border-top: 1px solid #eee; margin: 24px 0; }\n"
    "img { max-width: 320px; height: auto; }\n"
)


def _log(message: str) -> None:
    print(message)


def _format_wait_ms(wait_ms: int) -> str:
    seconds = wait_ms / 1000
    label = f"{seconds:.1f}".rstrip("0").rstrip(".")
    return f"{label}s"


def _wait_with_log(page, wait_ms: int, reason: str) -> None:
    if wait_ms <= 0:
        return
    _log(f"Waiting {_format_wait_ms(wait_ms)} to {reason}...")
    page.wait_for_timeout(wait_ms)


def _wait_for_tweet_detail(page, timeout_ms: int) -> object | None:
    if timeout_ms <= 0:
        return None
    try:
        with page.expect_response(
            lambda resp: "TweetDetail" in resp.url,
            timeout=timeout_ms,
        ) as response_info:
            pass
    except PlaywrightTimeoutError:
        return None
    try:
        return response_info.value.json()
    except Exception:
        return None


def _expand_show_more(article, page, *, wait_ms: int = SHOW_MORE_WAIT_MS) -> None:
    """Click inline "Show more" buttons to expand truncated tweet text."""
    if article is None or page is None:
        return
    clicked = False
    for label in SHOW_MORE_LABELS:
        try:
            buttons = article.get_by_role("button", name=label, exact=True)
        except Exception:
            continue
        try:
            count = buttons.count()
        except Exception:
            continue
        for idx in range(count):
            try:
                buttons.nth(idx).click(timeout=2000)
                clicked = True
                if wait_ms > 0:
                    page.wait_for_timeout(wait_ms)
            except Exception:
                continue

    if clicked:
        return

    for label in SHOW_MORE_LABELS:
        try:
            nodes = article.locator(f'text="{label}"')
        except Exception:
            continue
        try:
            count = nodes.count()
        except Exception:
            continue
        for idx in range(count):
            try:
                nodes.nth(idx).click(timeout=2000)
                if wait_ms > 0:
                    page.wait_for_timeout(wait_ms)
            except Exception:
                continue


def _read_article_text(
    article,
    tweet_url: str,
    *,
    page=None,
    anchor_handle=None,
    timeout_ms: int = 15000,
) -> str:
    tweet_id = _status_id_from_url(tweet_url)
    current = article
    last_exc: PlaywrightTimeoutError | None = None

    for _ in range(3):
        if anchor_handle is not None:
            try:
                text = anchor_handle.evaluate(
                    "el => el.closest('article, div[data-testid=\"tweet\"]').innerText"
                )
                if text:
                    return text
            except Exception:
                pass
        if page is not None and tweet_id:
            evaluated = _evaluate_article_text(page, tweet_id)
            if evaluated:
                return evaluated
        try:
            return current.inner_text(timeout=timeout_ms)
        except PlaywrightTimeoutError as exc:
            last_exc = exc
            try:
                content = current.text_content(timeout=5000)
            except PlaywrightTimeoutError:
                content = None
            if content:
                return content
            if page is None:
                break
            _wait_with_log(page, WAIT_MS, "retry tweet text")
            refreshed = _locate_tweet_article(page, tweet_url, timeout_ms=timeout_ms)
            if refreshed is None:
                break
            _expand_show_more(refreshed, page)
            current = refreshed

    if current is not None:
        try:
            return current.evaluate("el => el.innerText")
        except Exception:
            pass
    if last_exc:
        raise last_exc
    raise RuntimeError("Could not read tweet text.")


def _anchor_handle_for_tweet(page, tweet_url: str):
    if page is None:
        return None
    tweet_id = _status_id_from_url(tweet_url)
    if not tweet_id:
        return None
    selector = f"a[href*='/status/{tweet_id}']"
    try:
        return page.locator(selector).first.element_handle()
    except Exception:
        return None


def _evaluate_article_text(page, tweet_id: str) -> str | None:
    if page is None or not tweet_id:
        return None
    script = """el => el.closest('article, div[data-testid="tweet"]').innerText"""
    try:
        selector = f"a[href*='/status/{tweet_id}']"
        return page.locator(selector).first.evaluate(script)
    except Exception:
        return None


def _unique_pair_path(path: Path) -> Path:
    """Return a unique .md path that does not clash with existing md or html."""
    if not path.exists() and not path.with_suffix(".html").exists():
        return path

    stem = path.stem
    suffix = path.suffix
    parent = path.parent
    counter = 1
    while True:
        candidate = parent / f"{stem} ({counter}){suffix}"
        if not candidate.exists() and not candidate.with_suffix(".html").exists():
            return candidate
        counter += 1


# --- Markdown conversion helpers (adapted from docflow) ---
STAT_KEYWORDS = (
    "retweet",
    "retuits",
    "retuit",
    "repost",
    "republicaciones",
    "quotes",
    "citas",
    "likes",
    "me gusta",
    "favoritos",
    "bookmarks",
    "marcadores",
    "views",
    "visualizaciones",
    "impresiones",
    "replies",
    "respuestas",
    "shares",
    "compartidos",
    "guardados",
    "read repl",
    "leer resp",
)
STAT_NUMBER_RE = re.compile(r"^\d[\d.,]*(?:\s?[kmbKMB])?$")
TIME_RE = re.compile(r"\b\d{1,2}:\d{2}\b")
QUOTE_MARKERS = {"quote"}
QUOTE_MARKERS_JS = ", ".join(f'"{m}"' for m in sorted(QUOTE_MARKERS))
SHOW_MORE_LABELS = (
    "Show more",
    "Mostrar más",
    "Mostrar mais",
    "Ver más",
    "Ver mais",
    "Read more",
    "Leer más",
)
THREAD_MAX_MINUTES = 24 * 60
THREAD_MARKER_RE = re.compile(r"\bthread\b|\bhilo\b", re.IGNORECASE)


def rebuild_urls_from_lines(text: str) -> str:
    lines = text.splitlines()
    out: List[str] = []
    building_url = False

    for original_line in lines:
        stripped = original_line.strip()
        if stripped.startswith(("https://", "http://")):
            if ELLIPSIS in stripped:
                before, _, after = stripped.partition(ELLIPSIS)
                if before:
                    out.append(before)
                building_url = False
                remainder = after.lstrip()
                if remainder:
                    out.append(remainder)
            else:
                out.append(stripped)
                building_url = True
            continue
        if building_url:
            if not stripped or stripped.endswith(":"):
                out.append(original_line)
                building_url = False
                continue
            if ELLIPSIS in stripped:
                before, _, after = stripped.partition(ELLIPSIS)
                if before:
                    out[-1] = out[-1] + before
                building_url = False
                remainder = after.lstrip()
                if remainder:
                    out.append(remainder)
                continue
            out[-1] = out[-1] + stripped
        else:
            if stripped == ELLIPSIS:
                continue
            if stripped.startswith(ELLIPSIS):
                remainder = stripped.lstrip(ELLIPSIS).lstrip()
                if remainder:
                    out.append(remainder)
                continue
            out.append(original_line)
    return "\n".join(out)


def _safe_filename(name: str) -> str:
    cleaned = "".join(ch for ch in name if ch not in '<>:"/\\|?*#').strip()
    cleaned = " ".join(cleaned.split())
    return cleaned[:200] or "Tweet"


def _build_title(
    author_name: str | None,
    author_handle: str | None,
    *,
    kind: str = "Tweet",
) -> str:
    base = kind
    if author_name or author_handle:
        base += " by "
        if author_name:
            base += author_name
        if author_handle:
            base += f" ({author_handle})"
    return base


def _build_filename(url: str, author_handle: str | None) -> str:
    tweet_id = Path(urlparse(url).path).name or "tweet"
    handle = (author_handle or "tweet").lstrip("@") or "tweet"
    base = f"Tweet - {handle}-{tweet_id}"
    return f"{_safe_filename(base)}.md"


def _parse_iso_datetime(value: str | None) -> datetime | None:
    if not value:
        return None
    try:
        parsed = datetime.fromisoformat(value.replace("Z", "+00:00"))
    except ValueError:
        return None
    if parsed.tzinfo:
        return parsed.astimezone(timezone.utc).replace(tzinfo=None)
    return parsed


def _minutes_since(entry_time: str | None, anchor_time: str | None) -> float | None:
    entry_dt = _parse_iso_datetime(entry_time)
    anchor_dt = _parse_iso_datetime(anchor_time)
    if not entry_dt or not anchor_dt:
        return None
    return (anchor_dt - entry_dt).total_seconds() / 60


def _parse_twitter_created_at(value: str | None) -> datetime | None:
    if not value:
        return None
    try:
        parsed = datetime.strptime(value, "%a %b %d %H:%M:%S %z %Y")
    except ValueError:
        return None
    return parsed.astimezone(timezone.utc).replace(tzinfo=None)


def _minutes_between(entry_time: datetime | None, anchor_time: datetime | None) -> float | None:
    if not entry_time or not anchor_time:
        return None
    return (anchor_time - entry_time).total_seconds() / 60


def _extract_author_details(article) -> tuple[str | None, str | None]:
    author_name = None
    author_handle = None
    for txt in article.locator("span").all_text_contents():
        text = txt.strip()
        if not text:
            continue
        if text.startswith("@") and author_handle is None:
            author_handle = text
            continue
        if author_name is None and not text.startswith("@"):
            author_name = text
    return author_name, author_handle


def _extract_time_details(article) -> tuple[str | None, str | None]:
    time_el = article.locator("time").first
    if time_el.count() == 0:
        return None, None
    try:
        time_text = time_el.inner_text().strip()
    except Exception:
        time_text = ""
    time_datetime = time_el.get_attribute("datetime")
    return (time_text or None), time_datetime


def _resolve_thread_context(
    like_author_handle: str | None,
    like_time_text: str | None,
    like_time_datetime: str | None,
    target_author_handle: str | None,
    target_time_text: str | None,
    target_time_datetime: str | None,
) -> tuple[str | None, str | None, str | None]:
    return (
        like_author_handle or target_author_handle,
        like_time_text or target_time_text,
        like_time_datetime or target_time_datetime,
    )


def _extract_article_status_url(article, author_handle: str | None) -> str | None:
    hrefs = [
        anchor.get_attribute("href") or ""
        for anchor in article.locator("a[href*='/status/']").all()
    ]
    candidates = []
    for href in hrefs:
        canonical = _canonical_status_url(href)
        if canonical:
            candidates.append(canonical)
    if not candidates:
        return None
    if author_handle:
        handle = author_handle.lstrip("@")
        for candidate in candidates:
            if f"/{handle}/status/" in candidate:
                return candidate
    return candidates[0]


def _has_thread_marker(article) -> bool:
    try:
        link_texts = article.locator("a").all_text_contents()
    except Exception:
        return False
    for text in link_texts:
        if THREAD_MARKER_RE.search(text or ""):
            return True
    return False


def _select_thread_indices(
    entries: List[tuple[str | None, str | None, str | None]],
    target_idx: int | None,
    *,
    author_handle: str | None,
    time_text: str | None,
    anchor_time_datetime: str | None,
) -> List[int]:
    if target_idx is None or target_idx < 0 or target_idx >= len(entries):
        return []
    if not author_handle:
        return [target_idx]
    indices = [target_idx]
    idx = target_idx - 1
    while idx >= 0:
        handle, entry_time, entry_datetime = entries[idx]
        if handle != author_handle:
            break
        minutes = _minutes_since(entry_datetime, anchor_time_datetime)
        if minutes is not None:
            if 0 <= minutes <= THREAD_MAX_MINUTES:
                indices.append(idx)
                idx -= 1
                continue
            break
        if time_text and entry_time == time_text:
            indices.append(idx)
            idx -= 1
            continue
        break
    return sorted(indices)


def _collapse_blank_lines(lines: List[str]) -> List[str]:
    collapsed: List[str] = []
    previous_blank = True
    for line in lines:
        if not line:
            if previous_blank:
                continue
            collapsed.append("")
            previous_blank = True
            continue
        collapsed.append(line)
        previous_blank = False
    if collapsed and not collapsed[-1]:
        collapsed.pop()
    return collapsed


def _is_timestamp_line(line: str) -> bool:
    lower = line.lower()
    return bool(TIME_RE.search(line) and ("am" in lower or "pm" in lower))


def _is_keyword_stat(line: str) -> bool:
    lower = line.lower()
    return any(keyword in lower for keyword in STAT_KEYWORDS)


def _is_numeric_stat(line: str) -> bool:
    if line == MIDDLE_DOT:
        return True
    if STAT_NUMBER_RE.match(line):
        return True
    if line.lower().startswith("read ") and "repl" in line.lower():
        return True
    return False


def strip_tweet_stats(text: str) -> str:
    lines = [line.strip() for line in text.splitlines()]
    lines = _collapse_blank_lines(lines)

    while lines and not lines[-1]:
        lines.pop()

    while lines and (
        _is_timestamp_line(lines[-1])
        or _is_keyword_stat(lines[-1])
        or _is_numeric_stat(lines[-1])
    ):
        lines.pop()
        while lines and not lines[-1]:
            lines.pop()

    return "\n".join(lines).strip()


def _insert_quote_separator(text: str, quoted_url: str | None = None) -> str:
    """Insert a Markdown horizontal rule before quote markers."""
    lines = text.splitlines()
    out: List[str] = []
    inserted_link = False

    for line in lines:
        stripped = line.strip()
        if stripped.lower() in QUOTE_MARKERS:
            if not out or out[-1].strip() != "---":
                if out and out[-1].strip():
                    out.append("")
                out.append("---")
            if quoted_url and not inserted_link:
                out.append(f"[View quoted tweet]({quoted_url})")
                inserted_link = True
            out.append(line)
            continue
        out.append(line)

    return "\n".join(out)


def _insert_media_before_quote(text: str, media_lines: List[str]) -> str:
    if not media_lines:
        return text
    lines = text.splitlines()
    for idx, line in enumerate(lines):
        if line.strip().lower() in QUOTE_MARKERS:
            insert_at = idx
            for j in range(idx - 1, -1, -1):
                if lines[j].strip() == "---":
                    insert_at = j
                    break
            block: List[str] = []
            if insert_at > 0 and lines[insert_at - 1].strip():
                block.append("")
            block.extend(media_lines)
            block.append("")
            lines = lines[:insert_at] + block + lines[insert_at:]
            return "\n".join(lines)
    return text


def _is_after_quote_marker(img, root) -> bool:
    if root is None:
        return False
    try:
        return bool(
            img.evaluate(
                f"""
                (el, root) => {{
                    const markers = new Set([{QUOTE_MARKERS_JS}]);
                    const walker = document.createTreeWalker(
                        root,
                        NodeFilter.SHOW_TEXT
                    );
                    let quoteEl = null;
                    while (walker.nextNode()) {{
                        const value = walker.currentNode.nodeValue || "";
                        const text = value.trim().toLowerCase();
                        if (markers.has(text)) {{
                            quoteEl = walker.currentNode.parentElement;
                            break;
                        }}
                    }}
                    if (!quoteEl) return false;
                    const pos = el.compareDocumentPosition(quoteEl);
                    return !!(pos & Node.DOCUMENT_POSITION_PRECEDING);
                }}
                """,
                root,
            )
        )
    except Exception:
        return False


def _canonical_status_url(href: str | None) -> str | None:
    """Normalize a tweet URL by dropping suffixes (/photo, /analytics...)."""
    if not href or "/status/" not in href:
        return None
    absolute = href
    if not href.startswith(("http://", "https://")):
        absolute = urljoin("https://x.com", href)
    parsed = urlparse(absolute)
    segments = [seg for seg in parsed.path.split("/") if seg]
    if len(segments) >= 4 and segments[0] == "i" and segments[1] == "web" and segments[2] == "status":
        status_id = segments[3]
        if not status_id:
            return None
        return f"https://x.com/i/web/status/{status_id}"
    if len(segments) < 3 or segments[1] != "status":
        return None
    user = segments[0]
    status_id = segments[2]
    if not user or not status_id:
        return None
    return f"https://x.com/{user}/status/{status_id}"


def _status_id_from_url(url: str | None) -> str | None:
    if not url or "/status/" not in url:
        return None
    parsed = urlparse(url)
    segments = [seg for seg in parsed.path.split("/") if seg]
    if len(segments) >= 4 and segments[0] == "i" and segments[1] == "web" and segments[2] == "status":
        return segments[3] or None
    if len(segments) >= 3 and segments[1] == "status":
        return segments[2] or None
    return None


def _find_rest_id(payload: object) -> str | None:
    if isinstance(payload, dict):
        if "rest_id" in payload and payload["rest_id"]:
            return str(payload["rest_id"])
        for value in payload.values():
            found = _find_rest_id(value)
            if found:
                return found
    elif isinstance(payload, list):
        for item in payload:
            found = _find_rest_id(item)
            if found:
                return found
    return None


def _find_quoted_status_id(payload: object) -> str | None:
    if isinstance(payload, dict):
        if payload.get("quoted_status_id_str"):
            return str(payload["quoted_status_id_str"])
        if payload.get("quoted_status_id"):
            return str(payload["quoted_status_id"])
        if "quoted_status_result" in payload:
            found = _find_rest_id(payload["quoted_status_result"])
            if found:
                return found
        for value in payload.values():
            found = _find_quoted_status_id(value)
            if found:
                return found
    elif isinstance(payload, list):
        for item in payload:
            found = _find_quoted_status_id(item)
            if found:
                return found
    return None


def _quoted_url_from_graphql_id(quoted_id: str | None, tweet_url: str) -> str | None:
    if not quoted_id:
        return None
    tweet_id = _status_id_from_url(tweet_url)
    if tweet_id and quoted_id == tweet_id:
        return None
    return f"https://x.com/i/web/status/{quoted_id}"


def _pick_quoted_tweet_url(hrefs: List[str], tweet_url: str) -> str | None:
    tweet_canonical = _canonical_status_url(tweet_url) or tweet_url.rstrip("/")
    seen = {tweet_canonical.lower()}
    for href in hrefs:
        canonical = _canonical_status_url(href)
        if not canonical:
            continue
        lower = canonical.lower()
        if lower in seen:
            continue
        return canonical
    return None


def _extract_quoted_tweet_url(article, tweet_url: str) -> str | None:
    hrefs = [
        anchor.get_attribute("href") or ""
        for anchor in article.locator("a[href*='/status/']").all()
    ]
    return _pick_quoted_tweet_url(hrefs, tweet_url)


def _has_quote_marker(text: str) -> bool:
    return any(line.strip().lower() in QUOTE_MARKERS for line in text.splitlines())


def _attach_quoted_status_listener(page) -> dict[str, str | None]:
    quoted: dict[str, str | None] = {"id": None}

    def handle_response(response) -> None:
        if quoted["id"]:
            return
        url = response.url
        if "TweetResultByRestId" not in url and "TweetDetail" not in url:
            return
        try:
            payload = response.json()
        except Exception:
            return
        found = _find_quoted_status_id(payload)
        if found:
            quoted["id"] = found

    page.on("response", handle_response)
    return quoted


def _attach_tweet_detail_listener(page) -> dict[str, object | None]:
    detail: dict[str, object | None] = {"payload": None}

    def handle_response(response) -> None:
        if detail["payload"] is not None:
            return
        url = response.url
        if "TweetDetail" not in url:
            return
        try:
            payload = response.json()
        except Exception:
            return
        detail["payload"] = payload

    page.on("response", handle_response)
    return detail


def _extract_thread_ids_from_payload(
    payload: object | None,
    *,
    author_handle: str | None,
    anchor_time_datetime: str | None,
) -> List[str]:
    if not payload or not author_handle:
        return []
    handle = author_handle.lstrip("@").lower()
    anchor_dt = _parse_iso_datetime(anchor_time_datetime)
    if isinstance(payload, dict):
        data = payload.get("data") or {}
        convo = data.get("threaded_conversation_with_injections_v2") or {}
        instructions = convo.get("instructions") or []
    else:
        return []

    entries = []
    for inst in instructions:
        if isinstance(inst, dict) and inst.get("type") == "TimelineAddEntries":
            entries = inst.get("entries") or []
            break

    thread_ids: List[str] = []
    for entry in entries:
        if not isinstance(entry, dict):
            continue
        content = entry.get("content") or {}
        item = content.get("itemContent") or {}
        tweet_result = item.get("tweet_results", {}).get("result", {})
        if not isinstance(tweet_result, dict) or tweet_result.get("__typename") != "Tweet":
            continue
        user = tweet_result.get("core", {}).get("user_results", {}).get("result", {})
        user_core = (user.get("core") or {}) if isinstance(user, dict) else {}
        screen_name = user_core.get("screen_name")
        if not screen_name or screen_name.lower() != handle:
            continue
        created_at = tweet_result.get("legacy", {}).get("created_at")
        created_dt = _parse_twitter_created_at(created_at)
        minutes = _minutes_between(created_dt, anchor_dt)
        if minutes is not None and (minutes < 0 or minutes > THREAD_MAX_MINUTES):
            continue
        rest_id = tweet_result.get("rest_id")
        if rest_id:
            thread_ids.append(str(rest_id))
    return thread_ids


def _split_image_urls(image_urls: List[str]) -> Tuple[Optional[str], List[str]]:
    avatar = None
    media: List[str] = []
    for url in image_urls:
        if avatar is None and "profile_images" in url:
            avatar = url
            continue
        media.append(url)
    return avatar, media


def _strip_media_params(url: str) -> str:
    if "pbs.twimg.com/media" not in url:
        return url
    parsed = urlparse(url)
    params = [
        (k, v)
        for k, v in parse_qsl(parsed.query, keep_blank_values=True)
        if k.lower() == "format"
    ]
    clean_query = urlencode(params, doseq=True)
    return urlunparse(parsed._replace(query=clean_query))


def _media_markdown_lines(media_urls: List[str]) -> List[str]:
    lines: List[str] = []
    for idx, image_url in enumerate(media_urls, start=1):
        if "abs-0.twimg.com/emoji" in image_url:
            lines.append(
                f'<img src="{image_url}" alt="emoji {idx}" '
                'style="width:32px;height:auto;vertical-align:middle;" />'
            )
        else:
            clean_url = _strip_media_params(image_url)
            lines.append(f"[![image {idx}]({clean_url})]({clean_url})")
    return lines


def _resolve_storage_state(storage_state: Path | None) -> Path | None:
    if storage_state is None:
        return None
    path = storage_state.expanduser()
    if not path.exists():
        raise FileNotFoundError(
            f"storage_state not found at {path}. "
            "Create it with 'python -m playwright codegen --save-storage x_state.json https://x.com'."
        )
    return path


def _locate_tweet_article(page, tweet_url: str | None = None, *, timeout_ms: int = 15000):
    tweet_id = _status_id_from_url(tweet_url) if tweet_url else None
    target_selector = f"a[href*='/status/{tweet_id}']" if tweet_id else None
    try:
        if target_selector:
            page.wait_for_selector(
                f"article {target_selector}, div[data-testid='tweet'] {target_selector}",
                timeout=timeout_ms,
            )
        else:
            page.wait_for_selector("article, div[data-testid='tweet']", timeout=timeout_ms)
    except PlaywrightTimeoutError:
        return None
    if target_selector:
        article = page.locator("article").filter(has=page.locator(target_selector))
        if article.count() > 0:
            return article.first
        tweet = page.locator("div[data-testid='tweet']").filter(
            has=page.locator(target_selector)
        )
        if tweet.count() > 0:
            return tweet.first
    article = page.locator("article")
    if article.count() > 0:
        return article.first
    tweet = page.locator("div[data-testid='tweet']")
    if tweet.count() > 0:
        return tweet.first
    return None


def _extract_primary_link(article, tweet_url: str) -> str | None:
    """Return the first http(s) link different from the tweet itself."""
    seen: set[str] = set()
    tweet_lower = tweet_url.rstrip("/").lower()
    for anchor in article.locator("a").all():
        href = anchor.get_attribute("href") or ""
        expanded = (
            anchor.get_attribute("data-expanded-url")
            or anchor.get_attribute("data-full-url")
            or href
        )
        href = (expanded or "").strip()
        if not href or not href.startswith(("http://", "https://")):
            continue
        candidate = href.rstrip("/")
        lower = candidate.lower()
        if lower == tweet_lower or lower in seen:
            continue
        seen.add(lower)
        return candidate
    return None


def _extract_tweet_parts(
    article,
    tweet_url: str,
    *,
    page=None,
    quoted_status_id: str | None = None,
) -> TweetParts:
    anchor_handle = _anchor_handle_for_tweet(page, tweet_url) if page is not None else None
    author_name, author_handle = _extract_author_details(article)
    external_link = _extract_primary_link(article, tweet_url)

    quoted_tweet_url = None
    if quoted_status_id:
        quoted_tweet_url = _quoted_url_from_graphql_id(quoted_status_id, tweet_url)
    if not quoted_tweet_url:
        quoted_tweet_url = _extract_quoted_tweet_url(article, tweet_url)

    root_handle = None
    try:
        root_handle = article.element_handle()
    except PlaywrightTimeoutError:
        root_handle = None

    image_candidates: List[tuple[object, str]] = []
    seen: set[str] = set()
    for img in article.locator("img").all():
        src = img.get_attribute("src")
        candidate = None
        if src and "twimg.com" in src:
            candidate = src
        else:
            srcset = img.get_attribute("srcset")
            if srcset and "twimg.com" in srcset:
                parts = [p.strip() for p in srcset.split(",") if p.strip()]
                if parts:
                    candidate = parts[-1].split(" ")[0]
        if candidate and candidate not in seen:
            seen.add(candidate)
            image_candidates.append((img, candidate))

    if page is not None:
        _expand_show_more(article, page)

    raw_text = _read_article_text(
        article,
        tweet_url,
        page=page,
        anchor_handle=anchor_handle,
    )
    body_text = strip_tweet_stats(rebuild_urls_from_lines(raw_text).strip())

    has_quote_marker = _has_quote_marker(body_text)
    body_text = _insert_quote_separator(
        body_text,
        quoted_tweet_url if has_quote_marker and quoted_tweet_url else None,
    )

    image_urls_main: List[str] = []
    image_urls_quoted: List[str] = []
    for img, candidate in image_candidates:
        if has_quote_marker and root_handle and _is_after_quote_marker(img, root_handle):
            image_urls_quoted.append(candidate)
        else:
            image_urls_main.append(candidate)

    avatar_url, media_urls = _split_image_urls(image_urls_main)
    _, quoted_media_urls = _split_image_urls(image_urls_quoted)
    main_media_lines = _media_markdown_lines(media_urls)
    quoted_media_lines = _media_markdown_lines(quoted_media_urls)
    if has_quote_marker and main_media_lines:
        body_text = _insert_media_before_quote(body_text, main_media_lines)
        main_media_lines = []

    media_present = bool(media_urls or quoted_media_urls)
    trailing_media_lines = quoted_media_lines if has_quote_marker else main_media_lines

    return TweetParts(
        author_name=author_name,
        author_handle=author_handle,
        body_text=body_text,
        avatar_url=avatar_url,
        trailing_media_lines=trailing_media_lines,
        media_present=media_present,
        external_link=external_link,
    )


def _normalize_link_for_match(url: str) -> str:
    return url.strip().rstrip("/").lower()


def _should_append_external_link(body_text: str, external_link: str | None) -> bool:
    if not external_link:
        return False
    if not body_text:
        return True
    normalized = _normalize_link_for_match(external_link)
    return normalized not in body_text.lower()


def _build_single_tweet_markdown(parts: TweetParts, tweet_url: str) -> str:
    title = _build_title(parts.author_name, parts.author_handle)
    front_matter = [
        "---",
        "source: tweet",
        f"tweet_url: {tweet_url}",
    ]
    if parts.author_handle:
        front_matter.append(f'tweet_author: "{parts.author_handle}"')
    if parts.author_name:
        front_matter.append(f'tweet_author_name: "{parts.author_name}"')
    front_matter.extend(["---", ""])

    md_lines = [*front_matter, f"# {title}", "", f"[View on X]({tweet_url})"]
    if parts.avatar_url:
        md_lines.extend(["", f"![avatar]({parts.avatar_url})"])
    if parts.body_text:
        md_lines.extend(["", parts.body_text])
    if parts.trailing_media_lines:
        md_lines.append("")
        md_lines.extend(parts.trailing_media_lines)
        md_lines.append("")
    if _should_append_external_link(parts.body_text, parts.external_link):
        md_lines.extend(["", f"Original link: {parts.external_link}"])
    return "\n".join(md_lines).strip() + "\n"


def fetch_tweet_markdown(
    url: str,
    *,
    headless: bool = True,
    storage_state: Path | None = None,
) -> tuple[str, str]:
    if sync_playwright is None:
        raise RuntimeError(
            "playwright is not installed. Run 'pip install \"playwright>=1.55\"' and "
            "'playwright install chromium' to use this tool."
        )
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=headless)
        state_path = _resolve_storage_state(storage_state)
        context_kwargs = {"user_agent": USER_AGENT}
        if state_path:
            context_kwargs["storage_state"] = str(state_path)
        context = browser.new_context(**context_kwargs)
        if state_path:
            context.add_init_script(STEALTH_SNIPPET)
        page = context.new_page()
        quoted_status = _attach_quoted_status_listener(page)
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            _wait_with_log(page, WAIT_MS, "load the tweet")

            article = _locate_tweet_article(page, url)
            if article is None:
                raise RuntimeError(
                    "Could not find the post article. It may require login or be unavailable."
                )

            parts = _extract_tweet_parts(
                article,
                url,
                page=page,
                quoted_status_id=quoted_status["id"],
            )
            filename = _build_filename(url, parts.author_handle)
            markdown = _build_single_tweet_markdown(parts, url)
            return markdown, filename
        finally:
            context.close()
            browser.close()


def fetch_tweet_thread_markdown(
    url: str,
    *,
    headless: bool = True,
    storage_state: Path | None = None,
    like_author_handle: str | None = None,
    like_time_text: str | None = None,
    like_time_datetime: str | None = None,
) -> tuple[str, str]:
    if sync_playwright is None:
        raise RuntimeError(
            "playwright is not installed. Run 'pip install \"playwright>=1.55\"' and "
            "'playwright install chromium' to use this tool."
        )
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=headless)
        state_path = _resolve_storage_state(storage_state)
        context_kwargs = {"user_agent": USER_AGENT}
        if state_path:
            context_kwargs["storage_state"] = str(state_path)
        context = browser.new_context(**context_kwargs)
        if state_path:
            context.add_init_script(STEALTH_SNIPPET)
        page = context.new_page()
        quoted_status = _attach_quoted_status_listener(page)
        tweet_detail = _attach_tweet_detail_listener(page)
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            _wait_with_log(page, WAIT_MS, "load the tweet")

            article = _locate_tweet_article(page, url)
            if article is None:
                raise RuntimeError(
                    "Could not find the post article. It may require login or be unavailable."
                )

            target_parts = _extract_tweet_parts(
                article,
                url,
                page=page,
                quoted_status_id=quoted_status["id"],
            )
            target_time_text, target_time_datetime = _extract_time_details(article)
            effective_author_handle, effective_time_text, effective_time_datetime = (
                _resolve_thread_context(
                    like_author_handle,
                    like_time_text,
                    like_time_datetime,
                    target_parts.author_handle,
                    target_time_text,
                    target_time_datetime,
                )
            )
            filename = _build_filename(url, target_parts.author_handle)

            if not effective_author_handle or not (effective_time_text or effective_time_datetime):
                return _build_single_tweet_markdown(target_parts, url), filename

            thread_payload = tweet_detail.get("payload")
            if thread_payload is None:
                thread_payload = _wait_for_tweet_detail(page, TWEET_DETAIL_WAIT_MS)
                if thread_payload is not None:
                    tweet_detail["payload"] = thread_payload
            thread_marker = _has_thread_marker(article)
            thread_ids = _extract_thread_ids_from_payload(
                thread_payload,
                author_handle=effective_author_handle,
                anchor_time_datetime=effective_time_datetime,
            )

            articles = page.locator("article")
            total = articles.count()
            if total <= 1 and (not thread_ids or len(thread_ids) <= 1):
                if thread_payload is None and thread_marker:
                    _wait_with_log(page, WAIT_MS, "load the thread timeline")
                    thread_payload = tweet_detail.get("payload")
                    thread_marker = _has_thread_marker(article)
                    thread_ids = _extract_thread_ids_from_payload(
                        thread_payload,
                        author_handle=effective_author_handle,
                        anchor_time_datetime=effective_time_datetime,
                    )
                    articles = page.locator("article")
                    total = articles.count()
                if total <= 1 and (not thread_ids or len(thread_ids) <= 1):
                    return _build_single_tweet_markdown(target_parts, url), filename

            target_id = _status_id_from_url(url)
            target_idx = None
            if target_id:
                selector = f"a[href*='/status/{target_id}']"
                for idx in range(total):
                    if articles.nth(idx).locator(selector).count() > 0:
                        target_idx = idx
                        break

            entries: List[tuple[str | None, str | None, str | None]] = []
            for idx in range(total):
                article_handle = articles.nth(idx)
                _, author_handle = _extract_author_details(article_handle)
                time_text, time_datetime = _extract_time_details(article_handle)
                entries.append((author_handle, time_text, time_datetime))

            selected_indices = _select_thread_indices(
                entries,
                target_idx,
                author_handle=effective_author_handle,
                time_text=effective_time_text,
                anchor_time_datetime=effective_time_datetime,
            )

            if thread_ids and target_id and target_id in thread_ids:
                target_idx = thread_ids.index(target_id)

            if thread_ids and len(thread_ids) > len(selected_indices):
                primary_handle = effective_author_handle
                handle_slug = (primary_handle or "").lstrip("@")
                thread_parts: List[tuple[str | None, TweetParts]] = []
                for rest_id in thread_ids:
                    section_url = (
                        f"https://x.com/{handle_slug}/status/{rest_id}"
                        if handle_slug
                        else f"https://x.com/i/web/status/{rest_id}"
                    )
                    if target_id and rest_id == target_id:
                        thread_parts.append((section_url, target_parts))
                        continue
                    page.goto(section_url, wait_until="domcontentloaded", timeout=60000)
                    _wait_with_log(page, WAIT_MS, "load the thread tweet")
                    art = _locate_tweet_article(page, section_url)
                    if art is None:
                        continue
                    parts = _extract_tweet_parts(art, section_url, page=page)
                    thread_parts.append((section_url, parts))
            else:
                if len(selected_indices) <= 1:
                    return _build_single_tweet_markdown(target_parts, url), filename

                primary_handle = effective_author_handle
                thread_parts = []
                for idx in selected_indices:
                    art = articles.nth(idx)
                    section_url = url if idx == target_idx else _extract_article_status_url(
                        art, primary_handle
                    )
                    extract_url = section_url or url
                    parts = (
                        target_parts
                        if idx == target_idx
                        else _extract_tweet_parts(art, extract_url, page=page)
                    )
                    thread_parts.append((section_url, parts))

            if len(thread_parts) <= 1:
                return _build_single_tweet_markdown(target_parts, url), filename

            _log(f"Thread downloaded ({len(thread_parts)} tweets).")
            author_handle = effective_author_handle
            title = _build_title(target_parts.author_name, author_handle, kind="Thread")
            count = len(thread_parts)
            front_matter = [
                "---",
                "source: tweet",
                f"tweet_url: {url}",
                "tweet_thread: true",
                f"tweet_thread_count: {count}",
            ]
            if author_handle:
                front_matter.append(f'tweet_author: "{author_handle}"')
            if target_parts.author_name:
                front_matter.append(f'tweet_author_name: "{target_parts.author_name}"')
            front_matter.extend(["---", ""])

            md_lines = [*front_matter, f"# {title}"]
            if target_parts.avatar_url:
                md_lines.extend(["", f"![avatar]({target_parts.avatar_url})"])

            for section_url, parts in thread_parts:
                link_url = section_url or url
                md_lines.extend(["", "---", f"[View on X]({link_url})"])
                if parts.body_text:
                    md_lines.extend(["", parts.body_text])
                if parts.trailing_media_lines:
                    md_lines.append("")
                    md_lines.extend(parts.trailing_media_lines)
                    md_lines.append("")
                if _should_append_external_link(parts.body_text, parts.external_link):
                    md_lines.extend(["", f"Original link: {parts.external_link}"])

            markdown = "\n".join(md_lines).strip() + "\n"
            return markdown, filename
        finally:
            context.close()
            browser.close()


# --- Like-scraping helpers ---

def _normalize_stop_url(url: str | None) -> str | None:
    if not url:
        return None
    return _canonical_status_url(url.strip())


def _should_continue(collected: Sequence[object], max_tweets: int, stop_found: bool) -> bool:
    return len(collected) < max_tweets and not stop_found


def _extract_tweet_urls(page, seen: Set[str]) -> List[str]:
    urls: List[str] = []
    articles = page.locator("article")
    for article in articles.element_handles():
        links = article.query_selector_all("a[href*='/status/']")
        for link in links:
            href = link.get_attribute("href")
            canonical = _canonical_status_url(href)
            if not canonical or canonical in seen:
                continue
            seen.add(canonical)
            urls.append(canonical)
    return urls


def _extract_like_metadata(article) -> tuple[str | None, str | None, str | None, str | None]:
    author_name = None
    author_handle = None
    for span in article.query_selector_all("span"):
        try:
            text = (span.inner_text() or "").strip()
        except Exception:
            continue
        if not text:
            continue
        if text.startswith("@") and author_handle is None:
            author_handle = text
            continue
        if author_name is None and not text.startswith("@"):
            author_name = text
    time_el = article.query_selector("time")
    time_text = None
    time_datetime = None
    if time_el:
        try:
            time_text = (time_el.inner_text() or "").strip() or None
        except Exception:
            time_text = None
        time_datetime = time_el.get_attribute("datetime")
    return author_name, author_handle, time_text, time_datetime


def _extract_like_items(page, seen: Set[str]) -> List[LikeTweet]:
    items: List[LikeTweet] = []
    articles = page.locator("article")
    for article in articles.element_handles():
        link = article.query_selector("a:has(time)")
        href = link.get_attribute("href") if link else None
        canonical = _canonical_status_url(href)
        if not canonical:
            links = article.query_selector_all("a[href*='/status/']")
            for alt in links:
                alt_href = alt.get_attribute("href")
                canonical = _canonical_status_url(alt_href)
                if canonical:
                    break
        if not canonical:
            continue
        if canonical in seen:
            continue
        seen.add(canonical)
        author_name, author_handle, time_text, time_datetime = _extract_like_metadata(article)
        items.append(
            LikeTweet(
                url=canonical,
                author_handle=author_handle,
                author_name=author_name,
                time_text=time_text,
                time_datetime=time_datetime,
            )
        )
    return items


def collect_likes_from_page(
    page,
    likes_url: str,
    max_tweets: int,
    stop_at_url: str | None,
) -> Tuple[bool, int, List[str], bool, str | None]:
    _log(f"Loading {likes_url}...")
    page.goto(likes_url, wait_until="domcontentloaded", timeout=60000)
    try:
        page.wait_for_selector("article", timeout=15000)
    except PlaywrightTimeoutError:
        _log("No articles detected; the session may not be active.")
        return False, 0, [], False, _normalize_stop_url(stop_at_url)

    collected: List[str] = []
    seen: Set[str] = set()
    max_scrolls = 20
    idle_scrolls = 0
    stop_absolute = _normalize_stop_url(stop_at_url)
    stop_found = False
    articles = page.locator("article")

    while _should_continue(collected, max_tweets, stop_found):
        for url in _extract_tweet_urls(page, seen):
            collected.append(url)
            if stop_absolute and url == stop_absolute:
                stop_found = True
                break
            if not _should_continue(collected, max_tweets, stop_found):
                break
        if not _should_continue(collected, max_tweets, stop_found):
            break

        before = articles.count()
        page.mouse.wheel(0, 2000)
        page.wait_for_timeout(1500)
        after = articles.count()
        if after <= before:
            idle_scrolls += 1
            if idle_scrolls >= max_scrolls:
                break
        else:
            idle_scrolls = 0

    total_articles = articles.count()
    summary = (
        f"Likes loaded. Visible articles: {total_articles}. "
        f"URLs collected: {len(collected)} (limit: {max_tweets})"
    )
    if stop_absolute:
        summary += f". Stop URL {'found' if stop_found else 'not found'}."
    _log(summary)

    if collected:
        _log("URLs detected:")
        for idx, url in enumerate(collected, 1):
            _log(f"  {idx}. {url}")
    return True, total_articles, collected, stop_found, stop_absolute


def collect_like_items_from_page(
    page,
    likes_url: str,
    max_tweets: int,
    stop_at_url: str | None,
) -> Tuple[bool, int, List[LikeTweet], bool, str | None]:
    _log(f"Loading {likes_url}...")
    page.goto(likes_url, wait_until="domcontentloaded", timeout=60000)
    try:
        page.wait_for_selector("article", timeout=15000)
    except PlaywrightTimeoutError:
        _log("No articles detected; the session may not be active.")
        return False, 0, [], False, _normalize_stop_url(stop_at_url)

    collected: List[LikeTweet] = []
    seen: Set[str] = set()
    max_scrolls = 20
    idle_scrolls = 0
    stop_absolute = _normalize_stop_url(stop_at_url)
    stop_found = False
    articles = page.locator("article")

    while _should_continue(collected, max_tweets, stop_found):
        for item in _extract_like_items(page, seen):
            collected.append(item)
            if stop_absolute and item.url == stop_absolute:
                stop_found = True
                break
            if not _should_continue(collected, max_tweets, stop_found):
                break
        if not _should_continue(collected, max_tweets, stop_found):
            break

        before = articles.count()
        page.mouse.wheel(0, 2000)
        page.wait_for_timeout(1500)
        after = articles.count()
        if after <= before:
            idle_scrolls += 1
            if idle_scrolls >= max_scrolls:
                break
        else:
            idle_scrolls = 0

    total_articles = articles.count()
    summary = (
        f"Likes loaded. Visible articles: {total_articles}. "
        f"URLs collected: {len(collected)} (limit: {max_tweets})"
    )
    if stop_absolute:
        summary += f". Stop URL {'found' if stop_found else 'not found'}."
    _log(summary)

    if collected:
        _log("URLs detected:")
        for idx, item in enumerate(collected, 1):
            _log(f"  {idx}. {item.url}")
    return True, total_articles, collected, stop_found, stop_absolute


def fetch_likes_with_state(
    state_path: Path,
    *,
    likes_url: str,
    max_tweets: int,
    stop_at_url: str | None = None,
    headless: bool = True,
) -> Tuple[List[str], bool, int]:
    path = state_path.expanduser()
    if not path.exists():
        raise FileNotFoundError(
            f"storage_state not found at {path}. "
            "Create it with 'python -m playwright codegen --save-storage x_state.json https://x.com'."
        )

    if sync_playwright is None:
        raise RuntimeError("Install 'playwright>=1.55' to use this tool.")

    with sync_playwright() as playwright:
        try:
            browser = playwright.chromium.launch(headless=headless, channel="chrome")
        except Exception:
            browser = playwright.chromium.launch(headless=headless)
        context = browser.new_context(storage_state=str(path))
        context.add_init_script(STEALTH_SNIPPET)
        page = context.new_page()
        try:
            success, total, urls, stop_found, stop_absolute = collect_likes_from_page(
                page,
                likes_url=likes_url,
                max_tweets=max_tweets,
                stop_at_url=stop_at_url,
            )
            if not success:
                raise RuntimeError("Could not retrieve articles on the likes page.")
            if stop_found and stop_absolute and stop_absolute in urls:
                idx = urls.index(stop_absolute)
                urls = urls[:idx]
            return urls, stop_found, total
        finally:
            context.close()
            browser.close()


def fetch_like_items_with_state(
    state_path: Path,
    *,
    likes_url: str,
    max_tweets: int,
    stop_at_url: str | None = None,
    headless: bool = True,
) -> Tuple[List[LikeTweet], bool, int]:
    path = state_path.expanduser()
    if not path.exists():
        raise FileNotFoundError(
            f"storage_state not found at {path}. "
            "Create it with 'python -m playwright codegen --save-storage x_state.json https://x.com'."
        )

    if sync_playwright is None:
        raise RuntimeError("Install 'playwright>=1.55' to use this tool.")

    with sync_playwright() as playwright:
        try:
            browser = playwright.chromium.launch(headless=headless, channel="chrome")
        except Exception:
            browser = playwright.chromium.launch(headless=headless)
        context = browser.new_context(storage_state=str(path))
        context.add_init_script(STEALTH_SNIPPET)
        page = context.new_page()
        try:
            success, total, items, stop_found, stop_absolute = collect_like_items_from_page(
                page,
                likes_url=likes_url,
                max_tweets=max_tweets,
                stop_at_url=stop_at_url,
            )
            if not success:
                raise RuntimeError("Could not retrieve articles on the likes page.")
            if stop_found and stop_absolute:
                for idx, item in enumerate(items):
                    if item.url == stop_absolute:
                        items = items[:idx]
                        break
            return items, stop_found, total
        finally:
            context.close()
            browser.close()


# --- Markdown to HTML helpers ---

def split_front_matter(md_text: str) -> tuple[dict[str, str], str]:
    lines = md_text.splitlines()
    if not lines or lines[0].strip() != "---":
        return {}, md_text

    front_lines: list[str] = []
    for idx in range(1, len(lines)):
        if lines[idx].strip() == "---":
            meta = _parse_front_matter(front_lines)
            if not meta:
                return {}, md_text
            body = "\n".join(lines[idx + 1 :])
            if md_text.endswith("\n") and not body.endswith("\n"):
                body += "\n"
            return meta, body
        front_lines.append(lines[idx])

    return {}, md_text


def _parse_front_matter(lines: list[str]) -> dict[str, str]:
    meta: dict[str, str] = {}
    for line in lines:
        if ":" not in line:
            continue
        key, raw = line.split(":", 1)
        key = key.strip()
        value = raw.strip()
        if not key:
            continue
        if value.startswith(("\"", "'")) and value.endswith(("\"", "'")) and len(value) >= 2:
            value = value[1:-1]
        if value:
            meta[key] = value
    return meta


def front_matter_meta_tags(meta: dict[str, str]) -> str:
    tags: list[str] = []
    for key, meta_name in FRONT_MATTER_KEYS.items():
        if key not in meta:
            continue
        value = html.escape(str(meta[key]), quote=True)
        tags.append(f'<meta name="{meta_name}" content="{value}">')
    return "\n".join(tags) + ("\n" if tags else "")


def clean_duplicate_markdown_links(text: str) -> str:
    duplicate_link_pattern = r"\[(https?://[^\]]+)\]\(\1\)"

    def replace_duplicate_link(match):
        url = match.group(1)
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            path = parsed.path
            if len(path) > 30:
                path = path[:27] + "..."
            display_text = f"{domain}{path}"
            return f"[{display_text}]({url})"
        except Exception:
            return f"[View link]({url})"

    return re.sub(duplicate_link_pattern, replace_duplicate_link, text)


def convert_urls_to_links(text: str) -> str:
    text = clean_duplicate_markdown_links(text)

    lines = text.split("\n")
    processed_lines = []

    for line in lines:
        if "http" in line:
            url_pattern = r"https?://[^\s\)\]>\"']+"
            matches = list(re.finditer(url_pattern, line))

            for match in reversed(matches):
                url = match.group()
                start_pos = match.start()

                prefix = line[:start_pos].lower()

                prefix_lines = prefix.split("\n")
                is_in_markdown_link = "](" in prefix_lines[-1] if prefix_lines else False

                prefix_words = prefix.split()
                last_word = prefix_words[-1] if prefix_words else ""
                is_in_html_attribute = any(attr in last_word for attr in [
                    "href=", "src=", "srcset=", "poster=", "data-src=", "action=", "cite="
                ])
                is_in_css_url = "url(" in last_word

                if not (is_in_markdown_link or is_in_html_attribute or is_in_css_url):
                    line = line[:start_pos] + f"[{url}]({url})" + line[match.end():]

        processed_lines.append(line)

    return "\n".join(processed_lines)


def convert_newlines_to_br(html_text: str) -> str:
    def replace_in_content(match):
        tag_open = match.group(1)
        content = match.group(2)
        tag_close = match.group(3)

        content_with_br = content.replace("\n", "<br>\n")

        return f"{tag_open}{content_with_br}{tag_close}"

    html_text = re.sub(r"(<p[^>]*>)(.*?)(</p>)", replace_in_content, html_text, flags=re.DOTALL)
    html_text = re.sub(r"(<li[^>]*>)(.*?)(</li>)", replace_in_content, html_text, flags=re.DOTALL)
    html_text = re.sub(r"(<div[^>]*>)(.*?)(</div>)", replace_in_content, html_text, flags=re.DOTALL)

    return html_text


def extract_title(md_text: str) -> str:
    _, body = split_front_matter(md_text)
    for line in body.splitlines():
        stripped = line.strip()
        if stripped.startswith("# "):
            return stripped[2:].strip() or "Tweet"
    return "Tweet"


def markdown_to_html(md_text: str, title: str | None = None) -> str:
    if md_lib is None:
        raise RuntimeError("markdown is not installed. Run 'pip install markdown'.")

    meta, md_body = split_front_matter(md_text)
    md_body = convert_urls_to_links(md_body)

    try:
        html_body = md_lib.markdown(
            md_body,
            extensions=["fenced_code", "tables", "toc", "attr_list"],
            output_format="html5",
        )
    except Exception as exc:
        _log(f"Markdown conversion failed with extensions: {exc}")
        html_body = md_lib.markdown(md_body, output_format="html5")

    html_body = convert_newlines_to_br(html_body)
    html_title = title or extract_title(md_text)
    title_tag = f"<title>{html.escape(html_title)}</title>\n" if html_title else ""
    meta_tags = front_matter_meta_tags(meta)

    full_html = (
        "<!DOCTYPE html>\n"
        "<html>\n<head>\n<meta charset=\"UTF-8\">\n"
        f"{meta_tags}"
        f"{title_tag}"
        "<style>\n"
        f"{BASE_CSS}"
        "</style>\n"
        "</head>\n<body>\n"
        f"{html_body}\n"
        "</body>\n</html>\n"
    )

    return full_html


def _load_processed_urls(path: Path) -> List[str]:
    if not path.exists():
        return []
    lines = [line.strip() for line in path.read_text(encoding="utf-8").splitlines()]
    return [line for line in lines if line]


def _append_processed_urls(path: Path, urls: List[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    existing = _load_processed_urls(path)
    all_urls = list(urls) + [u for u in existing if u not in urls]
    path.write_text("\n".join(all_urls) + "\n", encoding="utf-8")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Download liked tweets and save Markdown + HTML files."
    )
    parser.add_argument(
        "--likes-url",
        default=DEFAULT_LIKES_URL,
        help="X likes URL (example: https://x.com/USER/likes)",
    )
    parser.add_argument(
        "--state-path",
        type=Path,
        default=Path(DEFAULT_STATE_PATH),
        help="Path to the storage_state exported after logging into X",
    )
    parser.add_argument(
        "--dest-dir",
        type=Path,
        default=Path(DEFAULT_DEST_DIR),
        help="Directory where Markdown and HTML are saved",
    )
    parser.add_argument(
        "--max-tweets",
        type=int,
        default=DEFAULT_MAX_TWEETS,
        help="Number of likes to capture in this run",
    )
    parser.add_argument(
        "--stop-at-url",
        help="Stop capture when this URL appears (useful to avoid duplicates)",
    )
    parser.add_argument(
        "--processed-file",
        type=Path,
        help="Path to the file that tracks processed URLs",
    )
    parser.add_argument(
        "--no-history",
        action="store_true",
        help="Do not read or write processed URL history",
    )
    headless_group = parser.add_mutually_exclusive_group()
    headless_group.add_argument(
        "--headless",
        dest="headless",
        action="store_true",
        default=True,
        help="Run Chromium in headless mode (default)",
    )
    headless_group.add_argument(
        "--no-headless",
        dest="headless",
        action="store_false",
        help="Open Chromium with UI (useful for debugging)",
    )
    return parser.parse_args()


def _resolve_history_path(dest_dir: Path, args: argparse.Namespace) -> Path | None:
    if args.no_history:
        return None
    if args.processed_file:
        return args.processed_file.expanduser()
    return dest_dir / "tweets_processed.txt"


def download_likes(args: argparse.Namespace) -> None:
    likes_url = (args.likes_url or "").strip()
    if not likes_url:
        raise SystemExit("--likes-url is required (example: https://x.com/USER/likes)")

    if md_lib is None:
        raise SystemExit("markdown is not installed. Run 'pip install markdown'.")

    if args.max_tweets <= 0:
        raise SystemExit("--max-tweets must be a positive integer")

    dest_dir: Path = args.dest_dir.expanduser()
    dest_dir.mkdir(parents=True, exist_ok=True)

    history_path = _resolve_history_path(dest_dir, args)
    existing_urls: set[str] = set()
    stop_at_url = args.stop_at_url
    if history_path:
        existing_list = _load_processed_urls(history_path)
        existing_urls = set(existing_list)
        if not stop_at_url and existing_list:
            stop_at_url = existing_list[0]

    state_path: Path = args.state_path.expanduser()
    if not state_path.exists():
        raise SystemExit(f"storage_state not found: {state_path}")

    try:
        items, stop_found, _ = fetch_like_items_with_state(
            state_path,
            likes_url=likes_url,
            max_tweets=args.max_tweets,
            stop_at_url=stop_at_url,
            headless=args.headless,
        )
    except Exception as exc:
        raise SystemExit(f"Could not read X likes: {exc}") from exc

    if stop_at_url and not stop_found:
        _log("Warning: stop URL not found in likes. Increase --max-tweets if needed.")

    if existing_urls:
        items = [item for item in items if item.url not in existing_urls]

    if not items:
        _log("No new tweets in your likes.")
        return

    saved_urls: List[str] = []

    for item in items:
        try:
            markdown, filename = fetch_tweet_thread_markdown(
                item.url,
                headless=args.headless,
                storage_state=state_path,
                like_author_handle=item.author_handle,
                like_time_text=item.time_text,
                like_time_datetime=item.time_datetime,
            )
        except Exception as exc:
            _log(f"Error processing {item.url}: {exc}")
            continue

        output_md = _unique_pair_path(dest_dir / filename)
        output_md.write_text(markdown, encoding="utf-8")
        output_html = output_md.with_suffix(".html")
        try:
            html_text = markdown_to_html(markdown)
        except Exception as exc:
            _log(f"Error converting {output_md.name} to HTML: {exc}")
            continue
        output_html.write_text(html_text, encoding="utf-8")
        saved_urls.append(item.url)
        _log(f"Saved {output_md.name} and {output_html.name}")

    if history_path and saved_urls:
        _append_processed_urls(history_path, saved_urls)


def main() -> None:
    args = parse_args()
    download_likes(args)


if __name__ == "__main__":
    main()
